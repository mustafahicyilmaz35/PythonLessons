{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8M61qvKMntR",
        "colab_type": "code",
        "outputId": "d99d11d9-4bb3-4b41-e9d7-149f94eb5b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#pytorch ile xor fonksiyonunu modelleyeceğiz.\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([[0,0],[0,1],[1,0],[1,1]]) #Girdimiz\n",
        "label = np.array([[0],[1],[1],[0]]) #Olması gereken çıktımız.\n",
        "\n",
        "#Networkü oluşturacak olan class ı oluşturcağız.Bu class nn.Module classından inherit olacak.\n",
        "class Network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Network,self).__init__()\n",
        "    #init fonksiyonuna hangi layerları tutacağını içinde kaç tane layer tutacağını söylemem gerekiyor.\n",
        "    #fullyconnected layer kullanacağız yani bizim ağlarımızın hepsi birbirlerine aynı derecede bağlı\n",
        "    #Yani ağırlıklar arasında herhangi bir atlama söz konusu değil.\n",
        "    self.layer_1 = nn.Linear(2,4) #Birinci katmana iki nöron giriş yaptı ve 4 çıktı yaptı.\n",
        "    self.layer_2 = nn.Linear(4,4) #Birinci katman 4 çıktı vermişti. Birinci katmanın çıktıları ikinci katmanın girdileri olacağı için, ikinci katmanın input değeri 4 olmuştur.\n",
        "    self.layer_3 = nn.Linear(4,1)\n",
        "\n",
        "    #Layerları oluşturduğumuza göre artık ileri yayılım yapabiliriz. Bunun için forward isimi bir fonksiyon oluşturacağız.\n",
        "    #parametre bu fonksiyon layerlardan gelen çıktıyla çalışacak, bu yüzden output_ parametresini de tanımladık.\n",
        "  def forward(self,output_):\n",
        "    #output_ u aktivasyon fonksiyonları ile şekillendirmeliyiz.\n",
        "    #Aktivasyon fonksiyonu olarak sigmoid i kullandık ve layer1 in output unu şekillendirdik.\n",
        "    output_ = F.sigmoid(self.layer_1(output_)) #relu fonksiyonlarını da aktivasyon fonksiyon olarak kullanıp şekillendirme yapabiliriz.\n",
        "    output_ = F.sigmoid(self.layer_2(output_))\n",
        "    output_ = F.sigmoid(self.layer_3(output_))\n",
        "    return output_\n",
        "#Network class ının objesini oluşturmalıyız.\n",
        "network = Network()\n",
        "#Optimizer kullanmalyız. SGD Stocastic Gradiant Descent\n",
        "optimizer = torch.optim.SGD(network.parameters(),lr=0.1,momentum=0.99)\n",
        "#loss function oluşturulmalı. nn nesneninden Mean Squared Error u çağırarak loss_func ı oluşturacağız.\n",
        "loss_func = nn.MSELoss()\n",
        "#epoch belirleyelim. Yani kaç kere veri setini dolaşacağını belirleyelim.\n",
        "epoch = 1000\n",
        "for epo in range(epoch):\n",
        "  #4 tane datamız olduğu için her birine id verebilmek için range i olarak belirledik.\n",
        "  for idx in range(4):\n",
        "    data_x = torch.tensor([data[idx]])\n",
        "    label_x = torch.tensor([label[idx]])\n",
        "    #optimizer zer_grad metodu ile hesaplamayı yapsın\n",
        "    optimizer.zero_grad()\n",
        "    #Gerçek çıktımızı alacağız.\n",
        "    network_output = network(data_x.float())\n",
        "    loss = loss_func(network_output,label_x.float()) # Gerçek değerimiz ile elde ettiğimiz değer arasındaki farkı buluyoruz.\n",
        "    #Geriye doğru yayılımı yapıyoruz.\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(network(torch.tensor([data[0]]).float()))\n",
        "print(network(torch.tensor([data[1]]).float()))\n",
        "print(network(torch.tensor([data[2]]).float()))\n",
        "print(network(torch.tensor([data[3]]).float()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0023]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.9991]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.9970]], grad_fn=<SigmoidBackward>)\n",
            "tensor([[0.0010]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}